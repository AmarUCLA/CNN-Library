# Convolutional Neural Networks

**Learning and Building Convolutional Neural Networks using PyTorch.**

### Content

<details>
  <summary> Convolutional Neural Networks</summary>
      ðŸ”¥ Basic ConvNet
      <br>
      ðŸ”¥ AlexNet
      <br>
      ðŸ”¥ VGGNet
      <br>
      ðŸ”¥ NIN
      <br>
      ðŸ”¥ GoogLeNet
      <br>
      ðŸ”¥ BatchNorm + ConvNet
      <br>
      ðŸ”¥ ResNet
      <br>
      ðŸ”¥ DenseNet
      <br>
      ðŸ”¥ Squeeze and Excitation Network
      <br>
      ðŸš€ EfficientNet Network
      <br>
      ðŸš€ MLPMixer Network
      <br>
</details>

### Create Environment
```python
python -m venv CNNs 
source CNNs/bin/activate 
```

### Installation
```python
pip install -r requirements.txt
```

### Run
```python
python main.py --model=resnet
```

**ðŸ”¥ Basic ConvNet**

  - Simple Convolutional Network with BatchNorm.

**ðŸ”¥ AlexNet**

  ![AlexNet Block](Images/alexnet.png)

  - AlexNet is widely remembered as the **breakthrough CNN architecture** on ImageNet dataset, but surprisingly its not the first CNN.

**ðŸ”¥ VGGNet**

  ![VGGNet Block](Images/vggnet.png)

  - It brought in the idea of buliding **a block of network** like a template unlike previous CNN architecture where the network is built layer by layer with increasing complexity.

**ðŸ”¥ NIN**

  ![NIN Block](Images/nin.png)

  - **Network In Network** introduced one of the key concept in deep neural network of **dimension downsampling/upsampling using 1x1Conv layer.**

**ðŸ”¥ GoogLeNet**

  ![GoogLeNet Block](Images/googlenet.png)

  - It combined ideas from NIN and VGG network introducing InceptionV1 also known as GoogLeNet. 

**ðŸ”¥ BatchNorm + ConvNet**

  ![BatchNorm Block](Images/batchnorm.png)

  - BatchNorm was introduced as a concept to **normalize the mini-batches traversing through the layer** and had an impactful results having **regularization** effect. But why BatchNorm is effective is quite unclear? the author suggests that BatchNorm reduce internal variant shift but other researchers  pointed out that the effects which batchNorm is effective against is not related to covariant shift. It is still widely discussed topic in DL.

**ðŸ”¥ ResNet**

  ![ResNet Block](Images/resnet.png)

  - ResNet Architecture has huge influence in current DNN architectures. It introduces the idea of **skip connection**, a concept of **adding** an unfiltered input to the conv layers.

**ðŸ”¥ DenseNet**

  ![DenseNet Block](Images/Densenet.png)

  - Building upon ResNet, DenseNet introduced the idea of **concatenating** the previous layers output and as well the inputs to the next layers.

**ðŸ”¥ Squeeze And Excitation Network**

  ![SENet Block](Images/senet.png)

  - Squeeze and Excitation Network models the interdependencies of channels in the images.
